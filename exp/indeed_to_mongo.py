
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/01_indeed_parse_to_mongo.ipynb

import requests
from bs4 import BeautifulSoup as bs
import pandas as pd
import re
from datetime import date, timedelta
from pymongo import MongoClient

def get_indeed_url_(what:str='software engineer', where:str='New York City, NY', radius:int=100):
    """
    what: <string> what type of job you're looking for
    where: <string> where you are searching for this job
    radius: <int> radius of job search
    """
    # formatting what query
    what_splitted = what.split(' ')
    if len(what_splitted) > 1: what = '+'.join(what_splitted)

    # formatting where query
    where_splitted = where.split(' ')
    if len(where_splitted) > 1:
        where_splitted = [s[0].upper()+s[1:] for s in where_splitted]
        where = "+".join(where_splitted)

    # creating url
    url = f'https://www.indeed.com/jobs?q={what}&l={where}&radius={str(radius)}'

    # returning
    return url

def parse_indeed_(url, pages=10):
    """
    will return
        title: title of the job posting,
        company: company that posted the job posting,
        location: location of the job,
        summary: summary of the job posting,
        date: date of the job posting
    for all pages available or 5 pages
    """
    # instantiating list os lists. Each sub array will be a page
    titles, companies, locations, summaries, dates = [], [], [], [], []

    for i in range(pages):
        # grabbing page contentw
        try:
            if i == 0: page = requests.get(url) # first page
            else: page = requests.get(f'{url}&start={i}0') # proceeding pages
            soup = bs(page.text, 'html.parser')

            # grabbing individual content and appending to our lists
            titles.append([tag.text.strip() for tag in soup.select('.jobtitle')])
            companies.append([tag.text.strip() for tag in soup.select('.company')])
            locations.append([tag.text.strip() for tag in soup.select('.location')])
            summaries.append([tag.text.strip() for tag in soup.select('.summary')])
            dates.append([tag.text.strip() for tag in soup.select('.date')])

        except: break

    # putting our data into a list of list
    data = [titles, companies, locations, summaries, dates]
    return data

def data_to_df(data):
    """
    convert raw data list of list into a pandas dataframe. We will use pandas df for any normalization and pre-processing of data
    """
    dfs = []
    for i in range(len(data)):
        s = pd.concat([pd.DataFrame(t) for t in data[i]])
        s = s.reset_index().drop('index', axis=1)
        dfs.append(s)
    # creating df
    df = pd.concat(dfs, axis=1)
    df.columns = ['title', 'company', 'location', 'summary', 'date']

    return df

def convert_to_date(dates):
    today = date.today()
    if dates == 'Today' or dates == 'Just': return pd.to_datetime(today)
    elif dates == '30+': return pd.to_datetime(today - timedelta(30))
    else: return pd.to_datetime(today - timedelta(int(dates)))

def convert_dates_(df, sort=True):
    ds = df['date'].str.split()
    ds = ds.apply(lambda x: x[0])
    ds = ds.apply(lambda x: convert_to_date(x))
    # converting to datetime
    df['date'] = ds

    # sort
    if sort:
        df.sort_values('date', ascending=False, inplace=True)
        df.reset_index(inplace=True)
        df.drop('index', axis=1, inplace=True)

def push_to_mongo(df, records):
    """
    pushed parsed df into our mongo client
    """
    for i in range(len(df)):
        jobslist = df.loc[i].to_dict()
        if records.find_one(jobslist): pass # does it exist? if so pass
        else: records.insert_one(jobslist)